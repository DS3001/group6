{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5MlZsykosil"
      },
      "outputs": [],
      "source": [
        "# This is the writing portion of the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlu8v6pqckCU"
      },
      "source": [
        "Project Concept:\n",
        "\n",
        "What is the research question? references to the codebook:\n",
        "\n",
        "- how often do people read the newspaper?\n",
        "- data: years: 1972,1975,1977,1978,1982,1983,195,1986,1987,1988,1989,1990,1991,1993,1994,1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2018,2021, and 2022\n",
        " and then we have the frequency categories: 1= every day, 2=a few times a week, 3= once a week, 4=less than once a week, and 5= never\n",
        " - and then there are the counts fro mthe frequency categories for each year\n",
        " - we do have the -100: inapplicable, -99: no answer, -98: don't know and -97: skipped\n",
        "\n",
        " - from this data we wanted to see how often people read the newspaper and after, if we can find any explanations:\n",
        " - 1. internet was invented in 1983 so maybe after this the frequency of reading a newspaper decreased bc people can find news from various sites / apps;\n",
        " - 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk5cYo4ngWDz"
      },
      "source": [
        "Trends from the data:\n",
        "\n",
        "- generally see an increase in the \"never\" option as the years increase\n",
        "- generally see a decrase in the \"everyday\" option as the years increase\n",
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOjgRYp7o3xr"
      },
      "source": [
        "Research strategy: how we did the project step by step\n",
        "\n",
        "1. Extracted the news data from the whole GSS dataset\n",
        "2. Cleaned the data\n",
        "- The data came in with each year having total rows for each frequency category. Ex- 1972 came in with 1,614 rows because that was all the counts from 1-5s and from the -110 to -97.\n",
        "  - so we made each year to have its own and one column with the friequecny categories and the count\n",
        "- After doing that, there were a lot of NAs, so we dropped them\n",
        "- After dropping the NAs, we dropped the -100, -99, -98, -97 because their codes are inaplicable, no answer, do not know, or skipped ( no substantial info from them)\n",
        "- After dropping the negative numbers, we wanted to have the frequency counts for each year ( individually), without any Nas or negative values\n",
        "- we then made all of the individual years with the frequency counts a dataset so we could graph them  \n",
        "3. Data Visualization\n",
        "- We graphed a grouped kernal density plot for each frequency category (1-5) for all the years. EX-\n",
        "- a plot with all the years for the people that read the newspaper \"everyday\",\n",
        "- another plot with all the years for the people that read the newspaper \"a few times a week\",\n",
        "-  a plot with all the years for the people that read the newspaper \"once a week\",  \n",
        "- a plot with all the years for the people that read the newspaper \"less than once a week\",\n",
        "-  a plot for all the years that people read the newspaper \"never\"\n",
        "\n",
        "\n",
        "- Then we made a linechart to see the number of people reading newspapers over time; y axis ahs the count of people, x axis has the years 1970s, 1980s, 1990, 2000, 2010,2020 ( decades) and then the lines are different colours for the frequencies: blue is for 1= everday, 2= a few times a week, 3= once a week, 4= less than once a week, 5= never\n",
        "4. Analyzed the results\n",
        "5. Wrote our summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URJqHZjnuI5U"
      },
      "source": [
        "Wrangling:\n",
        "\n",
        "- how are missing values handled?\n",
        "  - remove them because they do not help our data and also do not damage it if removed\n",
        "\n",
        "\n",
        "- for variables with large numbers of missing values, to what extend do the data and documentation provide an explanation for the missing data?\n",
        " - there are 25342/43504 missing values ( 58%)\n",
        " - According to the \"Suspected Issues in the GSS.pdf\", it says that it \"utilizes negative laues to denote missing values (e.g, don't know, inapplicable)\"\n",
        "  - so this means that the negative values are the missing values and they are from people anwering i dont know, inapplicable, or they just skip the question\n",
        "\n",
        "- if multiple data sources are used, how are the data merged?\n",
        "  - we only used the \"NEWS\" data from the whole GSS dataset.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
